# Run MHub

All Mhub models are bundled with all their dependencies in Docker containers.
This means that you need to have Docker installed on your system to run any of our models. In turn, you don't have to worry about setting up the right environment and installing the model dependencies on your system, as this is all done for you in the Docker containers. In addition, we package all MHub models in our MHub-IO framework, which provides a toolset for data operations. These data operations are executed in sequential order and form an MHub workflow.

In this document, we show how easy it is to run MHub models and how you can customize MHub workflows to meet your needs.

## Data preparation for MHub

All MHub models come with a standard workflow that runs a DICOM-to-DICOM pipeline. To run the default workflow, all you need to do is prepare a folder with your input data and a folder where the output generated by MHub will be stored.

*Note: For models that require multiple inputs, you will need to provide the input data in a specific structure. We try to standardize this as much as possible, but it will always depend heavily on the use case. However, you can always specify a custom configuration and modify `FileStructureImporter` accordingly to work with your existing file structure, provided it is sufficiently organized.*

Consider the following local file structure in this example:

```text
/your/local/data/
├── dicom/
│   ├── IMG0001.dcm
│   ├── IMG0002.dcm
│   ├── IMG0003.dcm
│   └── ...
└── output/
```

## Run an Mhub container

The command to run a Mhub container is a docker command that looks like this:

```bash
docker  run -t --rm --network=none \
        -v /your/local/data/dicom:/app/data/input_data:ro \
        -v /your/local/data/output:/app/data/output \
        mhubai/totalsegmentator:latest
```

Let's break this down:

- We start with the `docker run` command, because all MHub models are Docker containers.
- Then we specify the `-t` flag to tell Docker to assign a pseudo-tty.
- The `--rm` flag to tell Docker to not keep the container alive but to remove it after execution.
- The `--network none` flag to start the container without an Internet connection for extra security.

Then we need to specify the input data and the output location. To do this, we map folders from our machine (usually referred to as the host machine) to the Docker container using the `-v` argument, first specifying the full absolute path on our local machine, followed by the absolute path inside the Docker container, separated by a colon. We specify the optional `:ro` flag for the input mount to mount the input data into Docker for reading only. It is always a good practice to give as few permissions as necessary.

- The `-v /your/local/data/dicom:/app/data/input_data:ro` tells MHub that the input data can be found under `/your/local/data/dicom` and mounts this into the container read-only.
- The `-v /your/local/data/output:/app/data/output` gives the container write access to the folder `/your/local/data/output` and tells MHub to write all generated output here.

Finally, we need to specify the Docker image (`repo/image:tag`), where the repository is always `mhubai`, the image is the [model name](../mhub_models/model_json.md#name), and the tag is either `:latest`, `:stable`, or a specific stable version such as `:v1-m1` (more about [MHub versioning](./versioning.md) here).

- The `mhubai/totalsegmentator:latest` tells Docker to run the the latest version of the `totalsegmentator` model. If the image is not found locally, it will be pulled from the `mhubai` repository from Dockerhub automatically.

*You can run `docker -run --help` to see a full list of options*

## Inside the Container

There are several ways you can run our containers. We'll only briefly cover this topic, as it's more about using Docker containers in general. If you want to debug a Docker container, it can be helpful to go into the container and manually run the MHub pipeline. This way you can check the generated files without exporting them to your host machine.

To do this, you need to override the entrypoint of the container and start the container in interactive mode by adding `--entrypoint bash -i`.

To stick with our example from above, the command looks like this:

```bash
docker  run -it --rm --network=none \
        -v /your/local/data/dicom:/app/data/input_data:ro \
        -v /your/local/data/output:/app/data/output \
        --entrypoint bash \
        mhubai/totalsegmentator:latest
```

Once connected to the container, you can call the MHub pipeline manually:

```bash
# run inside of the container 
mhub.run --workflow default
```

## Tweak the Configuration

You can write your own workflows to customize the data flow within the container. For example, you can start from NIFTI files instead of DICOM files and export the raw model data directly or export additional reports or log files. For more details, see the articles about the [config file](../mhubio/the_mhubio_config_file.md) and [MHUb-IO modules](../mhubio/mhubio_modules.md).

Suppose you have written a custom workflow in the `custom.yml` configuration file:

```text
/your/local/data/
├── config/
│   └── custom.yml
├── dicom/
│   ├── IMG0001.dcm
│   ├── IMG0002.dcm
│   ├── IMG0003.dcm
│   └── ...
└── output/
```

Then you need to include the custom configuration in the container at `/app/models/$model_name/config/custom.yml`. Note that the name of the YAML configuration file is automatically the name of the workflow.

To start your custom workflow according to the example, execute:

```bash
docker  run -it --rm --network=none \
        -v /your/local/data/dicom:/app/data/input_data:ro \
        -v /your/local/data/output:/app/data/output \
        -v /your/local/data/config/custom.yml:/app/models/totalsegmentator/config/custom.yml \
        --entrypoint bash \
        mhubai/totalsegmentator:latest
        --workflow custom
```
